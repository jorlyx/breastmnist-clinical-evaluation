{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698c9c35-9d26-4bfd-90fc-29d09f8940e8",
   "metadata": {},
   "source": [
    "01_data_loading_and_inspection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5639d-4e30-4a6a-870b-21adbd0f2fa6",
   "metadata": {},
   "source": [
    "# Medical Imaging Dataset Inspection\n",
    "\n",
    "This notebook loads the BreastMNIST dataset and inspects:\n",
    "- dataset splits,\n",
    "- image shapes,\n",
    "- label structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9b07f-e5fb-4e12-90d2-d751ceb27d8e",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "\n",
    "We load the BreastMNIST dataset and create separate training, validation, and test splits.\n",
    "These splits ensure that model development and evaluation remain unbiased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a09cc-5132-4aa1-adc4-e2218acb8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from medmnist import BreastMNIST\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fb18f-d2c2-4749-8bd1-dc1bea00c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = BreastMNIST(split='train', transform=transform, download=True)\n",
    "val_dataset = BreastMNIST(split='val', transform=transform, download=True)\n",
    "test_dataset = BreastMNIST(split='test', transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f6e05-dcb5-4c18-9b1a-352aa8ef9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3a464-c7b0-4db5-8a93-361b3434cc1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Dataset exploration\n",
    "\n",
    "We inspect individual samples to understand image dimensions, label format,\n",
    "and visual characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3e42f-f5f7-4657-a72a-5bd8971bda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[0]\n",
    "\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da42d29-a0b0-4e66-9ffa-916ab5ce4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844de53-1e22-44de-a21c-841ef20e9b48",
   "metadata": {},
   "source": [
    "## 3. Class distribution\n",
    "\n",
    "We analyse the distribution of class labels across training, validation, and test splits\n",
    "to assess class imbalance and dataset representativeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1c6a5-e1bc-4cc0-b6ce-3fd98fccfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataset):\n",
    "    return dataset.labels.squeeze()\n",
    "\n",
    "y_train = get_labels(train_dataset)\n",
    "y_val = get_labels(val_dataset)\n",
    "y_test = get_labels(test_dataset)\n",
    "\n",
    "print(\"Unique labels (train):\", np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f57b73-b335-405e-85f0-2716b6e7c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(y):\n",
    "    labels, counts = np.unique(y, return_counts=True)\n",
    "    return dict(zip(labels.tolist(), counts.tolist()))\n",
    "\n",
    "train_counts = class_counts(y_train)\n",
    "val_counts   = class_counts(y_val)\n",
    "test_counts  = class_counts(y_test)\n",
    "\n",
    "print(\"Train counts:\", train_counts)\n",
    "print(\"Val counts:\", val_counts)\n",
    "print(\"Test counts:\", test_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc78237-39b9-41a7-a4c8-8f49bc73e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_proportions(counts_dict):\n",
    "    total = sum(counts_dict.values())\n",
    "    return {k: v / total for k, v in counts_dict.items()}\n",
    "\n",
    "print(\"Train proportions:\", class_proportions(train_counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca472e22-70ea-47b0-bf8d-5a79938ead2e",
   "metadata": {},
   "source": [
    "### Class distribution by split (to assess imbalance and representativeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b63884-06b0-441c-8ae7-d0205f54f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\", \"val\", \"test\"]\n",
    "counts_list = [train_counts, val_counts, test_counts]\n",
    "\n",
    "all_labels = sorted(set(y_train.tolist()) | set(y_val.tolist()) | set(y_test.tolist()))\n",
    "\n",
    "x = np.arange(len(all_labels))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i, (split, counts) in enumerate(zip(splits, counts_list)):\n",
    "    values = [counts.get(lbl, 0) for lbl in all_labels]\n",
    "    plt.bar(x + i*width, values, width=width, label=split)\n",
    "\n",
    "plt.xticks(x + width, all_labels)\n",
    "plt.xlabel(\"Class label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class distribution by split (BreastMNIST)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a153027-f150-44ee-a042-21346a106fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i, (split, counts) in enumerate(zip(splits, counts_list)):\n",
    "    values = [counts.get(lbl, 0) for lbl in all_labels]\n",
    "    plt.bar(x + i*width, values, width=width, label=split)\n",
    "\n",
    "plt.xticks(x + width, all_labels)\n",
    "plt.xlabel(\"Class label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class distribution by split (BreastMNIST)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = \"../results/class_distribution_by_split.png\"\n",
    "plt.savefig(out_path, dpi=200)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250d8af-1aec-4b21-8f6a-de805583ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts, val_counts, test_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893b8e4-e9f6-4979-ba7a-4702700c29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train counts:\", train_counts)\n",
    "print(\"Validation counts:\", val_counts)\n",
    "print(\"Test counts:\", test_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a2c0ce-6e9a-4b25-950b-3480683a4521",
   "metadata": {},
   "source": [
    "## 4. Visual characteristics of each class\n",
    "\n",
    "We visualise example images from each class to assess qualitative differences\n",
    "and verify that labels correspond to meaningful visual patterns.\n",
    "\n",
    "### Example images from each class (qualitative inspection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2528bba-d4f1-48d9-a607-835796e256b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples_for_class(dataset, class_id, n=8):\n",
    "    idxs = np.where(dataset.labels.squeeze() == class_id)[0][:n]\n",
    "    plt.figure(figsize=(n*1.2, 2))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img, lab = dataset[idx]\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(f\"Examples for class {class_id} (first {n}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_examples_for_class(train_dataset, class_id=0, n=8)\n",
    "show_examples_for_class(train_dataset, class_id=1, n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b4ebc-5040-4b29-a109-846388e1d736",
   "metadata": {},
   "source": [
    "### Pixel intensity histograms by class (quantitative comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36e235-496b-479b-ac9d-c49fd424fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intensity_histogram(dataset, class_id, n=100):\n",
    "    idxs = np.where(dataset.labels.squeeze() == class_id)[0][:n]\n",
    "    pixels = []\n",
    "    for idx in idxs:\n",
    "        img, _ = dataset[idx]\n",
    "        pixels.append(img.numpy().ravel())\n",
    "    pixels = np.concatenate(pixels)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(pixels, bins=50)\n",
    "    plt.title(f\"Pixel intensity histogram (class {class_id}, first {n} images)\")\n",
    "    plt.xlabel(\"Pixel intensity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    out_path = f\"../results/pixel_intensity_hist{class_id}.png\"\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "plot_intensity_histogram(train_dataset, class_id=0, n=100)\n",
    "plot_intensity_histogram(train_dataset, class_id=1, n=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa5f01-820e-4dbe-866f-1a68fbb7a974",
   "metadata": {},
   "source": [
    "## 5. Preprocessing and DataLoaders\n",
    "\n",
    "We define a preprocessing pipeline based on dataset statistics and construct DataLoaders\n",
    "to enable efficient and consistent batching during training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6a7bb-7c38-4a03-ab7d-638eb10dcb42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataset intensity statistics (mean and standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c1597-c094-4467-ab42-7fa4049b35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(dataset, n_samples=200):\n",
    "    pixels = []\n",
    "    for i in range(min(len(dataset), n_samples)):\n",
    "        img, _ = dataset[i]\n",
    "        pixels.append(img.numpy().ravel())\n",
    "    pixels = np.concatenate(pixels)\n",
    "    return pixels.mean(), pixels.std()\n",
    "\n",
    "mean, std = compute_mean_std(train_dataset)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868274df-5175-4fe2-a46b-ad81ca261fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[mean], std=[std])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b8990-6e0d-44c9-87ed-17c1ba77f8db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Batch inspection (shape and label verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb7e53-c17d-4d74-9b55-92ebd64d11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_proc = BreastMNIST(split=\"train\", transform=preprocess, download=False)\n",
    "val_dataset_proc = BreastMNIST(split=\"val\", transform=preprocess, download=False)\n",
    "test_dataset_proc = BreastMNIST(split=\"test\", transform=preprocess, download=False)\n",
    "\n",
    "len(train_dataset_proc), len(val_dataset_proc), len(test_dataset_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61100226-c220-426e-a344-da7439302840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_proc,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_proc,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset_proc,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013463a-b0ea-4b7a-932a-3fd261b9244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dde82d-612b-4c0b-ad76-f503d16e2a09",
   "metadata": {},
   "source": [
    "## 5. Baseline models\n",
    "\n",
    "We train simple, interpretable baseline models to establish reference performance\n",
    "before introducing more complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250a136-8b3d-45dc-b997-e18086d75945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dataset(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for img, label in dataset:\n",
    "        X.append(img.numpy().ravel())\n",
    "        y.append(int(label.item()))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = flatten_dataset(train_dataset_proc)\n",
    "X_val, y_val = flatten_dataset(val_dataset_proc)\n",
    "X_test, y_test = flatten_dataset(test_dataset_proc)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85a74e-cf82-406d-853c-b5046b836bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675ec7c-60bf-494d-8228-06bda554db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_val_pred = log_reg.predict(X_val)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"\\nClassification report (validation):\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be9356-7bfc-4fe8-92f2-eff2fb82ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion matrix - Logistic Regression (Validation)\")\n",
    "out_path = \"../results/cm_logreg_test.png\"\n",
    "plt.savefig(out_path, dpi=200)\n",
    "plt.show()\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd126a-6a1a-435a-a26d-eda4eb74b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_val_probs = log_reg.predict_proba(X_val)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_val, y_val_probs)\n",
    "auc = roc_auc_score(y_val, y_val_probs)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Logistic Regression (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af248b3a-4bf0-4f56-be66-737722fd481c",
   "metadata": {},
   "source": [
    "### Logistic Regression - Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b0ffc-d7c9-45c0-93bf-8bfafeb53299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion matrix - Logistic Regression (Test Validation)\")\n",
    "out_path = \"../results/cm_logreg_test.png\"\n",
    "plt.savefig(out_path, dpi=200)\n",
    "plt.show()\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0048d85-5389-4fac-9802-15d3c55c1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_test_probs = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_probs)\n",
    "auc = roc_auc_score(y_test, y_test_probs)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Logistic Regression (Test Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "out_path = \"../results/roc_logreg_test.png\"\n",
    "plt.savefig(out_path, dpi=200)\n",
    "plt.show()\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c98d0-eb81-4b34-9bb3-a63bab5aba0e",
   "metadata": {},
   "source": [
    "### CNN - Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5695d38-5166-4002-bfd4-296481fe939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e63cec-cbbf-41a5-b31b-95b61d34736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "\n",
    "        nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32 * 7 * 7, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cbff9-8748-4384-b44a-64064514b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af458a11-53ed-4062-b2a9-3b93af9d88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07751dea-0534-4a5f-9d03-ca4bd85b00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.squeeze().long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac81b0-3f59-4824-98a0-1b3d2c8433d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.squeeze().long().to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return running_loss / len(loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dea0d0-b8cd-4e1a-87e9-2d4f0850079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0b43f-4901-4701-8764-f55f0be3c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    history.append((epoch+1, train_loss, val_loss, val_acc))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Best Val Loss:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f8463-e38d-4645-9fa7-cfa10072c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_state)\n",
    "val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "print(\"Frozen best model -> Val Loss:\", val_loss, \"| Val Acc:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75035dee-f794-461c-9e58-be3c45424945",
   "metadata": {},
   "source": [
    "### CNN - Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d72f20-7c91-4947-aa41-170b24e00712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.squeeze().long().to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "y_test_true = np.concatenate(all_labels)\n",
    "y_test_pred = np.concatenate(all_preds)\n",
    "y_test_prob = np.concatenate(all_probs)\n",
    "\n",
    "print(\"Test accuracy:\", accuracy_score(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee09c8e-edc2-41ca-988b-5be82d998d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(y_test_true, y_test_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion matrix - SimpleCNN (Test)\")\n",
    "out_path = \"../results/cm_cnn_test.png\"\n",
    "plt.savefig(out_path, dpi=200)\n",
    "plt.show()\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db70823-d307-402d-bb07-610d8465bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test_true, y_test_prob)\n",
    "auc = roc_auc_score(y_test_true, y_test_prob)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - SimpleCNN (Test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "out_path = \"../results/roc_cnn_test.png\"\n",
    "plt.savefig(out_path, dpi=200)\n",
    "plt.show()\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "print(\"Test AUC:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66e274-7871-40af-9073-9328f2168821",
   "metadata": {},
   "source": [
    "## 7. Final Summary and Conclusions\n",
    "\n",
    "### Project Objective\n",
    "\n",
    "The aim of this project was to evaluate whether machine learning models could reliably classify breast imaging data into two categories. Particular focus was placed on clinically meaningful evaluation, including class imbalance, separability, threshold trade-offs, and generalisation performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Baseline Model – Logistic Regression\n",
    "\n",
    "A logistic regression classifier was implemented using flattened pixel intensities as a linear baseline.\n",
    "\n",
    "- Validation AUC: 0.860  \n",
    "- Test AUC: 0.797  \n",
    "\n",
    "While the model demonstrated meaningful separability, performance decreased on the test set, indicating moderate generalisation gap. Additionally, logistic regression does not capture spatial structure, which is fundamental in medical imaging.\n",
    "\n",
    "---\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    "\n",
    "A small CNN architecture was implemented to model spatial relationships via convolution and pooling.\n",
    "\n",
    "Early stopping was applied to prevent overfitting and freeze the best validation model.\n",
    "\n",
    "- Test AUC: 0.862  \n",
    "- Test accuracy: ~0.82  \n",
    "\n",
    "Although accuracy remained similar to logistic regression, the CNN significantly improved AUC, indicating stronger intrinsic class separability.\n",
    "\n",
    "---\n",
    "\n",
    "### Clinical Interpretation\n",
    "\n",
    "Improved AUC suggests that the CNN provides better ranking of diseased versus healthy images across thresholds. This allows greater flexibility when selecting operating points, such as prioritising sensitivity in screening contexts.\n",
    "\n",
    "However, minority class performance remains limited, and false negatives persist. Deployment would require:\n",
    "\n",
    "- Larger datasets  \n",
    "- External validation  \n",
    "- Calibration assessment  \n",
    "- Clinical risk tolerance analysis  \n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Small dataset size  \n",
    "- Class imbalance  \n",
    "- No external validation cohort  \n",
    "- Low-resolution (28×28) images compared to real mammography  \n",
    "\n",
    "These factors limit direct clinical applicability.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This project demonstrates that:\n",
    "\n",
    "1. Linear models provide a reasonable baseline but are limited by lack of spatial modelling.  \n",
    "2. Convolutional architectures significantly improve class separability.  \n",
    "3. AUC provides a more reliable assessment of model capability than accuracy alone.  \n",
    "4. Clinical evaluation must consider threshold trade-offs and generalisation, not just headline metrics.  \n",
    "\n",
    "The CNN model showed stronger intrinsic discrimination and would be preferred for further development, pending additional validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
